{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for experiments with two features, two agents, and the following allocation mechanisms:\n",
    "# baseline, fairness lottery, least fair, most compatible, product lottery, and static lottery\n",
    "# the only necessary modifications are:\n",
    "#   - choose whether to export dataframes as images (cell #3)\n",
    "#   - change choice_type string (cell #4)\n",
    "#   - change filepaths (cell #5)\n",
    "# generates 21 to 23 vizualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will save images of the NCDG & Proportional fairness tables\n",
    "# it requires an additional import to do so\n",
    "try:\n",
    "    import dataframe_image as dfi\n",
    "except:\n",
    "    %pip install dataframe_image\n",
    "    import dataframe_image as dfi\n",
    "GENERATE_PANDAS_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change choice_type to the appropriate choice mechanism to change image prefixez\n",
    "choice_type = \"choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to the appropriate paths\n",
    "BASELINE = '../data/history_file_baseline_2agents.json'\n",
    "FAIRNESS_LOTTERY = '../data/history_file_fairness_lottery_2agents.json'\n",
    "LEAST_FAIR = '../data/history_file_least_fair_2agents.json'\n",
    "MOST_COMPATIBLE = '../data/history_file_most_compatible_2agents.json'\n",
    "PRODUCT_LOTTERY = '../data/history_file_product_lottery_2agents.json'\n",
    "STATIC_LOTTERY = '../data/history_file_static_lottery_2agents.json'\n",
    "\n",
    "recs_file = '../data/recs.csv'\n",
    "items_file = '../data/item.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads recommender and item files\n",
    "recommender = pd.read_csv(recs_file, names=[\"User\",\"Item\",\"Score\"])\n",
    "item_features = pd.read_csv(items_file, names=[\"Item\",\"Feature\",\"BV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to change unless adding a new allocation mechanism\n",
    "history_files = [BASELINE,FAIRNESS_LOTTERY,LEAST_FAIR,MOST_COMPATIBLE,PRODUCT_LOTTERY,STATIC_LOTTERY]\n",
    "\n",
    "baseline = []\n",
    "fairness_lottery = []\n",
    "least_fair = []\n",
    "most_compatible = []\n",
    "product_lottery = []\n",
    "static_lottery = []\n",
    "\n",
    "list_names = [baseline,fairness_lottery,least_fair,most_compatible,product_lottery,static_lottery]\n",
    "readable_names = [\"Baseline\",\"Fairness Lottery\", \"Least Fair\", \"Most Compatible\", \"Product Lottery\", \"Static Lottery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for statistical analysis, change if adding new allocation mechanisms\n",
    "results_baseline = {}\n",
    "results_fairness_lottery = {}\n",
    "results_least_fair = {}\n",
    "results_most_compatible = {}\n",
    "results_product_lottery = {}\n",
    "results_static_lottery = {}\n",
    "\n",
    "dict_names = [results_baseline,results_fairness_lottery,results_least_fair,results_most_compatible,results_product_lottery,results_static_lottery]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read history files\n",
    "for history_file, list_name in zip(history_files,list_names):\n",
    "    with jsonlines.open(history_file) as reader:\n",
    "        for obj in reader:\n",
    "            list_name.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process history files into dictionaries for statistics\n",
    "for list, dictionary in zip(list_names, dict_names):\n",
    "    for line in list:\n",
    "        results = line['choice_out']['results']\n",
    "        results_list = []\n",
    "        for item in results:\n",
    "            results_list.append(item['item'])\n",
    "        dictionary[line['user']] = results_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTICS FUNCTIONS ###\n",
    "\n",
    "# get recommendation score based on user and item ids\n",
    "def lookupscore(user, item):\n",
    "    user = int(user)\n",
    "    item = int(item)\n",
    "    score = recommender.loc[(recommender.User == user) & (recommender.Item == item)][\"Score\"]\n",
    "    return float(score)\n",
    "\n",
    "# calculate ndcg given a list of recommended and ideal scores\n",
    "def ndcg(scores1, scores2):\n",
    "    idealdcg = 0.0\n",
    "    recdcg = 0.0\n",
    "    for index, val in enumerate(scores1): \n",
    "        recdcg += (2**val - 1)/np.log2(index + 2)\n",
    "    for index, val in enumerate(scores2):\n",
    "         idealdcg += (2**val - 1)/np.log2(index + 2)\n",
    "    return recdcg/idealdcg\n",
    "\n",
    "# not currently run in this file\n",
    "def plot_ndcg(name,ndcg_results):\n",
    "    ndcg_result = ndcg_results[name]\n",
    "    ndcg_data = pd.DataFrame(ndcg_result)\n",
    "    sb.lineplot(ndcg_data)\n",
    "    imagefile = name + \"ndcgplot.png\"\n",
    "    plt.savefig(imagefile)\n",
    "\n",
    "# given an item id return a list of its features as binary values\n",
    "def get_item_features(item_id):\n",
    "    feature_values = []\n",
    "    for value in item_features.loc[(item_features.Item == int(item_id))][\"BV\"]:\n",
    "        feature_values.append(value)\n",
    "    return feature_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VISUALIZATION FUNCTIONS ###\n",
    "# TODO: Update viz functions to make them more efficient and inline with statistics\n",
    "\n",
    "def process_history(history, fair=True, compat=True, alloc=True, lists=True):\n",
    "    if fair:\n",
    "        fair_list = [entry['allocation']['fairness scores'] for entry in history]\n",
    "        fair_df = pd.DataFrame(fair_list)\n",
    "    else:\n",
    "        fair_df = None\n",
    "    if compat:\n",
    "        compat_list = [entry['allocation']['compatibility scores'] for entry in history]\n",
    "        compat_df = pd.DataFrame(compat_list)\n",
    "    else:\n",
    "        compat_df = None\n",
    "    if alloc:\n",
    "        alloc_list = [entry['allocation']['output'] for entry in history]\n",
    "        alloc_df = pd.DataFrame(alloc_list)\n",
    "        alloc_df['none'] = (alloc_df['1'] == 0) & (alloc_df['2'] == 0)\n",
    "    else:\n",
    "        alloc_df = None\n",
    "    if lists:\n",
    "        results_list = [process_results(entry['choice_out']['results']) for entry in history]\n",
    "    else:\n",
    "        results_list = None\n",
    "    return fair_df, compat_df, alloc_df, results_list\n",
    "\n",
    "def process_results(result_structs):\n",
    "    return [(entry['item'], entry['score']) for entry in result_structs]\n",
    "\n",
    "def plot_fairness_time(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness\")\n",
    "    sb.lineplot(data=fair_df)\n",
    "    image_file = image_prefix + '-fairness.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_allocation(experiment_data, include_none=False, image_prefix=None):\n",
    "    alloc_df = pd.DataFrame(experiment_data[2])\n",
    "    if include_none is False:\n",
    "        if not alloc_df['none'][1:].any():\n",
    "            alloc_df.drop('none', axis=1, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Allocation\")\n",
    "    sb.lineplot(data=alloc_df.cumsum())\n",
    "    image_file = image_prefix + '-allocation.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_fairness_regret(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    regret = 1-fair_df\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness Regret\")\n",
    "    sb.lineplot(data=regret.cumsum())\n",
    "    image_file = image_prefix + '-regret.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def do_plots(experiment_data, include_none=False, image_prefix=None):\n",
    "    plot_fairness_time(experiment_data, include_none, image_prefix)\n",
    "    plot_allocation(experiment_data, include_none, image_prefix)\n",
    "    plot_fairness_regret(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process(experiment, include_none=False, image_prefix=None):\n",
    "    experiment_data = process_history(experiment)\n",
    "    do_plots(experiment_data, include_none, image_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates average NDCG for each allocation mechanism and stores them in the list avg_of_ndcg\n",
    "# also creates ndcg_results, a dictionary that allows plotting NDCG over time for each mechanism\n",
    "avg_of_ndcg = []\n",
    "ndcg_results = {}\n",
    "for dictionary, name in zip(dict_names, readable_names):\n",
    "    users = []\n",
    "    ndcg_values = []\n",
    "    for user, items in dictionary.items():\n",
    "        scores = []\n",
    "        for item in items:\n",
    "            scores.append(lookupscore(user, item))\n",
    "        ideal_scores = []\n",
    "        for score in recommender.loc[(recommender.User == int(user))][\"Score\"].sort_values(ascending=False):\n",
    "            ideal_scores.append(score)\n",
    "            ideal_scores = ideal_scores[0:len(scores)+1]\n",
    "        ndcg_values.append(ndcg(scores, ideal_scores))\n",
    "        users.append(user)\n",
    "    avg_of_ndcg.append(mean(ndcg_values))\n",
    "    ndcg_results[name] = {\"Users\":users, \"NDCG\":ndcg_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe of average NDCG values\n",
    "ndcg_table = pd.DataFrame(data=avg_of_ndcg, index=readable_names, columns=[\"NDCG\"])\n",
    "ndcg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates proportional fairness for the representation of item features\n",
    "# currently designed to handle two item features\n",
    "# TODO: make it easier to change number of item features\n",
    "either_fairness = []\n",
    "f1_representation = []\n",
    "f2_representation = []\n",
    "\n",
    "for dictionary, name in zip(dict_names, readable_names):\n",
    "    item_counter = 0\n",
    "    f0 = 0\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    either = 0\n",
    "    for items in dictionary.values():\n",
    "        for item in items:\n",
    "            item_counter += 1\n",
    "            list_of_features = get_item_features(item)\n",
    "            if list_of_features[0] == 1:\n",
    "                f0 += 1\n",
    "            if list_of_features[1] == 1:\n",
    "                f1 += 1\n",
    "            if list_of_features[2] == 1:\n",
    "                f2 += 1\n",
    "            if list_of_features[1] == 1 or list_of_features[2] == 1:\n",
    "                either += 1\n",
    "    either_fairness.append(either/item_counter)\n",
    "    f1_representation.append(f1/item_counter)\n",
    "    f2_representation.append(f2/item_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_results = {\"Protected\":either_fairness,\"1\":f1_representation,\"2\":f2_representation}\n",
    "prop_fairness_results = pd.DataFrame(data=fairness_results, index=readable_names)\n",
    "prop_fairness_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process history files for visualizations\n",
    "# TODO: identify necessary features & streamline this process \n",
    "fair_df_baseline, compat_df_baseline, alloc_df_baseline, results = process_history(baseline)\n",
    "fair_df_fairness_lottery, compat_df_fairness_lottery, alloc_df_fairness_lottery, results = process_history(fairness_lottery)\n",
    "fair_df_least_fair, compat_df_least_fair, alloc_df_least_fair, results = process_history(least_fair)\n",
    "fair_df_most_compatible, compat_df_most_compatible, alloc_df_most_compatible, results_most_compatible = process_history(most_compatible)\n",
    "fair_df_product_lottery, compat_df_product_lottery, alloc_df_product_lottery, results_product_lottery = process_history(product_lottery)\n",
    "fair_df_static_lottery, compat_df_static_lottery, alloc_df_static_lottery, results_static_lottery = process_history(static_lottery)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DF of fairness values\n",
    "# streamline as part of above visualization overhaul\n",
    "fair_df = pd.DataFrame()\n",
    "fair_df.loc[:,'Baseline'] = fair_df_baseline.sum(axis=1)\n",
    "fair_df.loc[:,'Fairness Lottery'] = fair_df_fairness_lottery.sum(axis=1)\n",
    "fair_df.loc[:,'Least Fair'] = fair_df_least_fair.sum(axis=1)\n",
    "fair_df.loc[:,'Most Campatible'] = fair_df_most_compatible.sum(axis=1)\n",
    "fair_df.loc[:,'Product Lottery'] = fair_df_product_lottery.sum(axis=1)\n",
    "fair_df.loc[:,'Static Lottery'] = fair_df_static_lottery.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate individual visualizations for allocation mechanisms\n",
    "for list, name in zip(list_names,readable_names):\n",
    "    prefix = choice_type + \"-\" + name.lower().replace(\" \",\"_\")\n",
    "    process(list, image_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of regret values to visualize all mechanisms together\n",
    "# update line 6 if there are more than two agents\n",
    "# TODO: change function order, creation of sum to handle more than two agents\n",
    "regrets = []\n",
    "for mechanism, name in zip(list_names, readable_names):\n",
    "    fair_df = process_history(mechanism)[0]\n",
    "    regret = 1-fair_df\n",
    "    regret[\"Allocation\"] = name\n",
    "    regret[\"Sum\"] = regret[\"1\"]+regret[\"2\"]\n",
    "    for row_num in range(len(regret)):\n",
    "        regret.loc[row_num, \"Agent 1 Regret\"] = regret.loc[0:row_num, \"1\"].sum()\n",
    "        regret.loc[row_num, \"Agent 2 Regret\"] = regret.loc[0:row_num, \"2\"].sum()\n",
    "        regret.loc[row_num, \"Total Regret\"] = regret.loc[0:row_num, \"Sum\"].sum()\n",
    "    regrets.append(regret)\n",
    "regret_all = pd.concat(regrets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "plt.xlabel(\"Users\")\n",
    "plt.ylabel(\"Fairness Regret\")\n",
    "sb.lineplot(x=regret_all.index,y=regret_all[\"Agent 1 Regret\"], hue=regret_all[\"Allocation\"])\n",
    "savename = choice_type + \"-all-regret-agent1.png\"\n",
    "plt.savefig(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "plt.xlabel(\"Users\")\n",
    "plt.ylabel(\"Fairness Regret\")\n",
    "sb.lineplot(x=regret_all.index,y=regret_all[\"Agent 2 Regret\"], hue=regret_all[\"Allocation\"])\n",
    "savename = choice_type + \"-all-regret-agent2.png\"\n",
    "plt.savefig(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 10))\n",
    "plt.xlabel(\"Users\")\n",
    "plt.ylabel(\"Fairness Regret\")\n",
    "sb.lineplot(x=regret_all.index,y=regret_all[\"Total Regret\"], hue=regret_all[\"Allocation\"])\n",
    "savename = choice_type + \"-all-regret-sum.png\"\n",
    "plt.savefig(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_fairness_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may return error: Chrome executable not able to be found on your machine\n",
    "# if this happens, add a chrome_path parameter with a path to Google Chrome on your machine\n",
    "#   Ex.\n",
    "#       dfi.export(df, name, chrome=path=\"/Users/Applications/Google Chrome\")\n",
    "if GENERATE_PANDAS_IMAGES:\n",
    "    dfi.export(ndcg_table, choice_type+\"-ndcg_table.png\")\n",
    "    dfi.export(prop_fairness_results, choice_type+\"-proportional_fairness_table.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
